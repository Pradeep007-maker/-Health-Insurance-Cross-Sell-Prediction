{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1685268624950}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - ** Health Insurance Cross Sell Prediction**\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Classification\n","##### **Contribution**    - Individual\n","##### **Team Member 1 -**D G V S PRADEEP\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["***Business problem***: Our client is an Insurance agency that has given Health care coverage to its clients, presently they need our assistance in building a model to foresee whether the policyholders (clients) from past year will likewise be keen on Vehicle Protection given by the organization. To foresee whether the client would be keen on Vehicle insurance, we have data about socioeconomics (orientation, age, district code type), Vehicles (Vehicle Age, Harm), Contract (Premium, obtaining channel) and so forth.\n","\n","First we import the vital libraries and check out at our information and its qualities. We have a dataset of 3,81,109 lines and 12 segments with no copy/missing information. Next we concentrate on the highlights completely and the information it addresses.\n","\n","In information fighting, we initially make a reinforcement dataset under the name, 'df_copy'. Then, at that point, We drop the 'id' segment as it isn't a lot of purpose for us. Afterward, we convert the qualities in straight out sections 'Driving_License' and 'Previously_Insured' from 1 and 0 to Yes and No for better perception. We undercover 'Region_Code' , 'Annual_Premium' and 'Policy_Sales_Channel' segments from float to int datatype to save space.\n","\n","Later we imagine our information and perform univariate investigation, bivariate examination concerning the dependant variable. The bits of knowledge found from each diagram is portrayed. At last, we imagine the relationship heatmap and pairplot for better comprehension.\n","\n","In light of our perceptions, we figure out 3 theoretical articulations and perform speculation tests. The assertions are:\n","\n","The typical yearly charge for a vehicle protection is more prominent than 20,000. The typical age of the client is more prominent than 30. The Standard deviation of yearly premium is 10,000.\n","\n","This present time was the opportunity for include designing. We took care of the exceptions in 'Annual_Premium' section by utilizing the covering technique. In the later boxplot we could envision every one of the exceptions eliminated. We wouldn't give vehicle protection to somebody who didn't have a permit to drive. Along these lines, we dropped 'driving permit' section as they are not giving any significant data. Furthur, we performed one hot encoding on our absolute highlights with dropping the primary section being valid. The information was imbalanced, where the dependant variable, 'Reaction' had 46,710 mark 1 passages and 3,34,399 name 0 enteries. This imbalanced dataset was adjusted utilizing Destroyed strategy. Next we scaled our information utilizing MinMax scaler. At last we split our information into train and test in 80-20 proportion.\n","\n","The information was prepared to squeeze into an AI model. To start with, we utilized Strategic Relapse Classifier. We got f1 score as 82%. Second, we utilized Irregular Backwoods Classifier. We got f1 score as 87.32%. We involved GridSearchCV for hyperparameter tuning in which we saw a slight improvement. Third, We involved XG-Lift with GridSearchCV for hyperparameter tuning. We got a f1 score as 84%.\n","\n","Out of all above models Irregular woodland Classifier gives the most noteworthy F1-score of 87% for test Set. No overfitting is seen.\n","\n","So at long last, the insurance agency can send an AI model that utilizes Irregular Woods Classifier to foresee the wheather the generally existing health care coverage client would be keen on a vehicle protection item. The organization can further develop the transformation rate by doing whatever it may take to urge individuals to purchase vehicle protection by offering a few motivations/simplicity of use and guarantee settlement process. Strategically pitching may be a compelling method for expanding the benefits since the client procurement cost actually stays 0."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["**Our client is an Insurance agency that has given Medical coverage to its clients, presently they need our assistance in **building a model to foresee whether the policyholders (clients) from past year will likewise be keen on Vehicle Protection gave by the company**.i.**"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required. \n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","# Importing Numpy & Pandas for data processing & data wrangling\n","import numpy as np\n","import pandas as pd\n","\n","# Importing  tools for visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Importing libraries for hypothesis testing\n","from scipy.stats import uniform\n","from scipy.stats import norm\n","from scipy.stats import chi2\n","from scipy.stats import t\n","from scipy.stats import f\n","\n","# Importing libraries for data pre-processing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV\n","from imblearn.over_sampling import SMOTE\n","\n","# Importing Machine Learning algorithm libraries\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","\n","# Importing Classification algorithm metrics\n","from sklearn import metrics\n","from mlxtend.plotting import plot_confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import classification_report"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tTp1_riia4I1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading Dataset to variable 'df'\n","df = pd.read_csv('/content/drive/MyDrive/ALMA PROJECT DATA/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv')"],"metadata":{"id":"oQuz_ycmbKRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","df.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","df.shape"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","print(len(df[df.duplicated()]))"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","# Checking the sum of null values for each column\n","df.isna().sum()"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["We have the dataset of health insurance customers and also the information \n","regarding their vehicle.\n","\n","The target variable is 'Response'.\n","\n","The dataset has 3,81,109 entries and 14 columns. Out of the 14 columns, 3 are of 'object' datatype and rest are of 'numeric' datatype.\n","\n","There are no missing values/duplicate values in the dataset"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","df.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","df.describe()"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description "],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["Answer Here\n","*  id: Unique row id\n","\n","*   gender: Gender of the customer\n","\n","*   Age: Age of the customer\n","\n","*   Driving_Licence: Whether the customer holds a DL or not\n","*   Region_Code: Unique code fot the region of the cusotmer\n","\n","*   Previously_Insured: Whether the customer previously had an insurance or not\n","\n","\n","*  Vehicle_Age: Age of the vehicle\n","\n","\n","*   Vehicle_Damage: Whether the vehicle had past damages or not.\n","\n","* Annual_Premium: The annual premium customer has to pay.\n","\n","*   Policy_Sales_Channel: Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n","*   Vintage: Number of Days, Customer has been associated with the company\n","\n","\n","*   Response: Customer is interested or not\n","\n","\n","\n","\n","\n"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","df.nunique()"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# Copying the dataset for backup\n","df_copy = df.copy()\n","\n","# We can drop the 'id' column as it is not much of use for us\n","df.drop(['id'],axis=1, inplace=True)"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting 'Driving_License' & 'Previously_Insured' from 1,0 to Yes and No.\n","\n","df['Driving_License'] = df['Driving_License'].apply(lambda x: 'Yes' if x==1 else \"No\")\n","df['Previously_Insured']=df['Previously_Insured'].apply(lambda x: 'Yes' if x==1 else \"No\")"],"metadata":{"id":"hlp7vHcNdWEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"3dyXxvjddf1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Coverting 'Region_Code'\t, 'Annual_Premium' &\t'Policy_Sales_Channel' from float to int datatype to save space.\n","\n","df['Region_Code']=df.Region_Code.astype(int)\n","df['Annual_Premium']=df.Annual_Premium.astype(int)\n","df['Policy_Sales_Channel']=df.Policy_Sales_Channel.astype(int)"],"metadata":{"id":"2IkGBUFtdkEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Divide Data in categorical and numerical features\n","\n","numeric_features= df.select_dtypes(exclude='object')\n","categorical_features=df.select_dtypes(include='object')"],"metadata":{"id":"MrRGg0okdmz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking out some numerical features\n","numeric_features.head()"],"metadata":{"id":"kHZwSZ84dpJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking out some numerical features\n","categorical_features.head()"],"metadata":{"id":"QRj-aVPXdtaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["Answer Here.\n","\n","*   First we create a backup dataset under the name, 'df_copy'.\n","\n","*   We drop the 'id' column as it is not much of use for us.\n","*   We convert the values in categorical columns 'Driving_License' & 'Previously_Insured' from 1 & 0 to Yes and No for better visualization.\n","\n","\n","*   We covert 'Region_Code' , 'Annual_Premium' & 'Policy_Sales_Channel' columns from float to int datatype to save space.\n","\n"],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"code","source":["sns.set(rc={'figure.figsize':(6,4)})"],"metadata":{"id":"zYB6LlIAeAY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set_style(style='white')"],"metadata":{"id":"1-X__XOleA82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","# Plotting a countplot for categorical variable- 'Response'\n","\n","sns.countplot(x=df['Response'], data=df, palette='mako')"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["Countplot is one of the best way to visualize the value count distribution of a categorical variable."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Out of the total respondants i.e 3,81,109 people, only 12.25% of (46,710) people were interested in buying the vehicle insurance from our company."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["The gained insight sheds the light on the cross selling conversion rate of the company which is close to 12%. The company can improve the conversion rate by taking steps to encourage people to buy vehicle insurance by offering some incentives/ease of application & claim settlement process. Cross selling might be an effective way to increase the profits since the customer acquisition cost still remains 0."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# Plotting a countplot for categorical variable- 'Previously_Insured'\n","\n","sns.countplot(x=df['Previously_Insured'], data=df, palette='mako')"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["Countplot is one of the best way to visualize the value count distribution of a categorical variable."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Out of the total respondants i.e 3,81,109 people, 54% (2,06,481) people had no previous vehicle insurance."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["As the majority of the people had no previous vehicle insurance, the company gets access to an untapped market, thus creating a postive business impact."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","# Plotting a countplot for categorical variable- 'Vehicle_Age'\n","\n","sns.countplot(x=df['Vehicle_Age'], data=df, palette='mako')"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["Countplot is one of the best way to visualize the value count distribution of a categorical variable."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["The majority of the vehicles possessed by the customers if in age range, < 1 year or 1-2 years."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["This can have a positive impact because, as most of them are young vehicles, the risk beared by the company is minimal. Also, the premium rates for newer vehicles are lower thus, making it more likely for the people to buy."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","# Plotting a piechart for categorical variable- 'Vehicle_Damage'\n","\n","plt.pie(df['Vehicle_Damage'].value_counts(),labels = df['Vehicle_Damage'].value_counts().keys().tolist(),autopct='%.0f%%')\n","plt.title('Vehicle_Damage')\n","plt.show()"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["Pie charts are one of the best ways for univariate analysis of categorical data."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["Half the health insurance customers have previously damaged vehicles."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["This might lead to negative business impact because, 40-50% claim rate in vehicle insurance industry can have huge financial impact on the company."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","# Plotting histplot for 'Age' column and visualzing mean (green line) and median (red line)\n","\n","sns.histplot(df['Age'],kde=True,color='darkslateblue',bins=np.arange(df['Age'].min(), df['Age'].max() + 1))\n","plt.axvline(df['Age'].mean(), color='g', linestyle='dashed', linewidth=2)\n","plt.axvline(df['Age'].median(), color='red', linestyle='dashed', linewidth=2)"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["Histogram along with the KDE line lets us visualize the density & the distribution of the feature. The mean & medain dashed lines also gives us the level of skewness."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["The first majority og the prople are in their 20s and the second majority are in their 40s."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["This can have a positive business impact because, as the majority of the customers' age is in range 20-30, they would have low health insurance premium rates, therby giving them opportunity to buy a vehicle insurance as well."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# Plotting histplot for 'Annual_Premium' column and visualzing mean (green line) and median (red line)\n","\n","df['Annual_Premium'].hist(bins = 50, density = True, range=[0, 80000],color='darkslateblue')\n","plt.axvline(df['Annual_Premium'].mean(), color='g', linestyle='dashed', linewidth=2)\n","plt.axvline(df['Annual_Premium'].median(), color='red', linestyle='dashed', linewidth=2)"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["Histogram along with the KDE line lets us visualize the density & the distribution of the feature. The mean & medain dashed lines also gives us the level of skewness."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["The annual premium of 2630 has the most frequency (64,877 entries) later which we can observe a normal distribution. This might indicate that this premium would not be of comprehensive type."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Yes, the company knows the annual premium they have to offer and they can utilize this to market these different types of vehicle insurances better for different people according to their needs."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# Plotting histplot for 'Policy_Sales_Channel' column and visualzing mean (green line) and median (red line)\n","\n","sns.histplot(x=df['Policy_Sales_Channel'],kde=True,color='darkslateblue')\n","plt.axvline(df['Policy_Sales_Channel'].mean(), color='g', linestyle='dashed', linewidth=2)\n","plt.axvline(df['Policy_Sales_Channel'].median(), color='red', linestyle='dashed', linewidth=2)"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the top 20 policy channels\n","df['Policy_Sales_Channel'].value_counts().head(20)"],"metadata":{"id":"TwLgDIDggZyN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["Histogram along with the KDE line lets us visualize the density & the distribution of the feature. The mean & medain dashed lines also gives us the level of skewness."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["The data shows that policy channel 152 is bringing most of it's customers for the company, followed by policy channel 26 & 124."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["This chart helps the marketing team for better understanding their sales channels, and also help them decide the medium to allocate funds for even better performance."],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","# Plotting a boxplot for 'Vintage' column \n","\n","plt.figure(figsize=(6,6))\n","sns.boxplot(df['Vintage'],palette='mako')"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["The Box plot chart helps in getting an all-round view of price distribution across neighbourhood groups."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["The boxplot suggests that there are customers ranging from 80-220 days are associated with the company, with the median age being 150 days."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["The data suggests, the longer customers are associated with the company, the more likely they are ging to buy a vehicle insurance product."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","# Plotting bartplot for 'Response' vs 'Gender' column \n","\n","sns.countplot(x='Gender',hue='Response',palette=\"mako\", data=df)"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["Countplot is one of the best way to visualize the relationship of a categorical variable wrt another categorical variable."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["\n","The data suggests there ares slightly more male customers compared to female customers which also reflects on the cross sell rate."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["This column cannot be considered for the ML algorithm therefore no positive/negative business impact is seen."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","# Plotting bartplot for 'Response' vs 'Previously_Insured' column \n","\n","sns.countplot(x='Previously_Insured',hue='Response',palette=\"mako\", data=df)"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Previously_Insured'].value_counts()"],"metadata":{"id":"iOGU0LX-hEq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["Countplot is one of the best way to visualize the relationship of a categorical variable wrt another categorical variable."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["The customers who previously did not have a vehicle insurance had a cross sell conversion rate approximately of 20%. The customers with existing vehicle insurance had zero conversion rate."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["As there's no conversion rate for customres who already had a previous insurance, the company can work on providing more competitive rates, to bring in more revenue"],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["# Chart - 11 visualization code\n","# Plotting bartplot for 'Response' vs 'Vehicle_Age' column\n","\n","sns.countplot(x='Vehicle_Age',hue='Response',palette=\"mako\", data=df)"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["Countplot is one of the best way to visualize the relationship of a categorical variable wrt another categorical variable."],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["The better performing range is 1-2 years, where we can see a higher positive response compared to the other 2 classes."],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["This info can again help the marketing team to better target customers with vehicle age range of 1-2 years, thus having a positive business impact."],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["# Chart - 12 visualization code\n","# Plotting bartplot for 'Response' vs 'Vehicle_Damage' column\n","\n","sns.countplot(x='Vehicle_Damage',hue='Response',palette=\"mako\", data=df)"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Vehicle_Damage'].value_counts()"],"metadata":{"id":"vEDC9RMLhyhP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["Countplot is one of the best way to visualize the relationship of a categorical variable wrt another categorical variable."],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["\n","We can observe from the data that only the customers' with previous vehicle damage have shown interest in the vehicle insurance."],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["This observation can lead to negative business growth as no previous damage customersare converting. This suggests that comprehensive premium might be higher than average."],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["# Chart - 13 visualization code\n","# Plotting bartplot for 'Response' vs 'Age' column\n","\n","df.groupby('Response').Age.hist(range=[20, 80], align = 'mid', bins=60, figsize=(15,8), density = True, alpha = 0.5)\n","plt.xlabel('Age')\n","plt.ylabel('Frequency')\n","plt.title('Age Histogram')\n","plt.legend(['Response = 0', 'Response = 1'])\n","plt.show()"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["A frequency histogram plot is used here to represent both Response & Age variables. This overlap lets us visualize the range of values of Age for which the Response would be Yes or No."],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["As we can see from the plot, the first age bracket i.e customers in their 20s are not showing interest in the product whereas, the second age cluster i.e cutomers in their 40s are showing much more interest towards the vehicle insurance product."],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact? \n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["This insight also inturn suggests that the initial premium rates are higher because of which younger customers are not showing interest, whereas customers in older age bracket think this product is of value, therefore preferring it over the competitors"],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Correlation Heatmap visualization code\n","\n","plt.figure(figsize=(15,8))\n","sns.heatmap(df.corr(),annot=True,cmap=\"crest\")\n","plt.show()"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["'Policy_Sales_channel' has the strongest (negative) correlation with 'Age'"],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["it appears that there is a strong negative correlation between the variables \"Policy_Sales_channel\" and \"Age.\" A negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa."],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot "],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","plt.figure(figsize=(15,8))\n","sns.pairplot(df, corner=True)"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["A pair plot is commonly used to visualize pairwise relationships between multiple variables in a dataset"],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["To gain insights from a chart or visualization, it is necessary to analyze the data displayed in the chart, understand the variables involved, and consider the context or domain to which the data belongs"],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["1.The average annual premium for a vehicle insurance is greater than 20,000.\n","\n","2.The average age of the customer is greater than 30.\n","\n","3.The Standard deviation of annual premium is 10,000."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["The average annual premium for a vehicle insurance is greater than 20,000.\n","\n","*   \n","  Null hypothesis H0: Average Annual premium not > 20,000.\n","*  Alternate hypothesis Ha: Average Annual premium > 20,000.\n","\n","\n","\n","\n"],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","annual_premium_sample = df['Annual_Premium'].sample(500)\n","annual_premium_mean = np.mean(annual_premium_sample)\n","annual_premium_std = np.std(annual_premium_sample)"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing test statistic\n","\n","ts = (annual_premium_mean-20000)/(annual_premium_std/(np.sqrt(500)))\n","print(ts)"],"metadata":{"id":"XEM-D2dak4o6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating the probability\n","\n","prob_z = norm.cdf(14.62, 0, 1)\n","print(prob_z)\n","\n","# P-Value\n","p1 = 1-prob_z\n","print(p1)"],"metadata":{"id":"clRTz2nxk7lP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["We have chosen Z-test to obtain p-value."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["As we are performing hypothesis testing for mean, we have chosen Z-test to obtain p-value. The probability we have obtained is close to 100%, so we have sufficient evidence to reject H0. Therefore, the average anuual premium is greater than 20,000."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["The average age of the customer is greater than 30.\n","\n","\n","*  Null hypothesis H0: Average age not > 30.\n","*   \n","Alternate hypothesis Ha: Average age > 30.\n","\n"],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","age_sample = df['Age'].sample(500)\n","age_mean = np.mean(age_sample)\n","age_std = np.std(age_sample)"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing test statistic\n","\n","ts = (age_mean-30)/(age_std/(np.sqrt(500)))\n","print(ts)"],"metadata":{"id":"90450TKclShP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating the probability\n","\n","prob_z = norm.cdf(13.48, 0, 1)\n","print(prob_z)\n","\n","# P-Value\n","p1 = 1-prob_z\n","print(p1)"],"metadata":{"id":"LH5boyLhlSnl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["We have chosen Z-test to obtain p-value."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["As we are performing hypothesis testing for mean, we have chosen Z-test to obtain p-value. The probability we have obtained is close to 100%, so we have sufficient evidence to reject H0. Therefore, the average age f the customer is greater than 30.\n"],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["The Standard deviation of annual premium is 10,000.\n","\n","\n","\n","*  Null hypothesis H0: Standard deviation != 10,000\n","*   \n","Alternate hypothesis Ha: Standard deviation = 10,000.\n","\n"],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","ap_sample = df['Annual_Premium'].sample(50)\n","S2 = (np.std(ap_sample))**2"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing test statistic\n","\n","ts3 = (49 * S2)/(10000*10000)\n","print(ts3)"],"metadata":{"id":"jkk647mulwwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating the probability\n","prob = chi2.cdf(158.82,49)\n","print(prob)"],"metadata":{"id":"oUBCinDflw5i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["We have chosen Chi2-test to obtain p-value."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["As we are performing hypothesis testing for standard deviation, we have chosen Chi2-test to obtain p-value. The probability we have obtained is 99.99%, so we have sufficient evidence to reject H0. Therefore, the standard deviation of annual premium is 10,000"],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["There are no missing values."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n","# Handling Outliers & Outlier treatments\n","# Plotting the boxplot for 'Annual_Premium'\n","\n","sns.boxplot(x=df['Annual_Premium'])"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding the IQR\n","\n","percentile25 = df['Annual_Premium'].quantile(0.25)\n","percentile75 = df['Annual_Premium'].quantile(0.75)\n","iqr = percentile75 - percentile25\n","upper_limit = percentile75 + 1.5 * iqr\n","lower_limit = percentile25 - 1.5 * iqr"],"metadata":{"id":"QGjsGRuamM7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Capping\n","# Capping the data above the upper limit to upper limit & below the lower limit to the lower limit\n","\n","df['Annual_Premium'] = np.where(\n","    df['Annual_Premium'] > upper_limit,\n","    upper_limit,\n","    np.where(\n","        df['Annual_Premium'] < lower_limit,\n","        lower_limit,\n","        df['Annual_Premium']\n","    )\n",")"],"metadata":{"id":"qcFZkIuvmOrO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting the boxplot again to check for outliers\n","sns.boxplot(x=df['Annual_Premium'])"],"metadata":{"id":"4EUbryszmUoT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["I have used 'Capping' method to treat outliers. As there are only 3,81,109 entries in my dataset, trimming the outliers would lead to data loss."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["# Encode your categorical columns\n","\n","# Using Pandas get Dummies for Encoding categorical features \n","df=pd.get_dummies(df,drop_first=True,sparse=True)"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"iJ6F2XF5mi6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["\n","I have used One hot encoding, and also dropping the first column of each encoded column. This method is an effective technique used to represent categorical variables as numerical values for a machine learning model."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing \n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction\n","contractions = {\n","    \"I'm\": \"I am\",\n","    \"can't\": \"cannot\",\n","    \"won't\": \"will not\",\n","    # Add more contractions and their expanded forms as needed\n","}\n","\n","def expand_contractions(text):\n","    words = text.split()\n","    expanded_words = [contractions.get(word, word) for word in words]\n","    expanded_text = \" \".join(expanded_words)\n","    return expanded_text\n","\n","# Example usage\n","contracted_text = \"I'm going to the store. It won't take long.\"\n","expanded_text = expand_contractions(contracted_text)\n","print(expanded_text)\n"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","text = \"The Quick Brown Fox JUMPS Over The LAZY Dog\"\n","lowercased_text = text.lower()\n","print(lowercased_text)\n"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","import string\n","\n","text = \"Hello, how are you?\"\n","no_punctuation_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n","print(no_punctuation_text)\n"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","import re\n","\n","text = \"I found a great website at https://www.example.com. Check it out!\"\n","no_url_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n","print(no_url_text)\n"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","text = \"I have 3 apples and 2 oranges. The code is ABC123.\"\n","no_digits_text = re.sub(r\"\\w*\\d\\w*\", \"\", text)\n","print(no_digits_text)\n"],"metadata":{"id":"YjwUwhzoozqv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n"],"metadata":{"id":"MJDY6ghDptJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove Stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","\n","text = \"This is an example sentence with some stopwords.\"\n","stop_words = set(stopwords.words('english'))\n","\n","# Tokenize the text\n","tokens = nltk.word_tokenize(text)\n","\n","# Remove stopwords\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","\n","# Join the filtered tokens back into a sentence\n","filtered_text = ' '.join(filtered_tokens)\n","\n","print(filtered_text)\n"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","text = \"    This is an example    sentence with   extra spaces.   \"\n","\n","# Remove leading and trailing white spaces\n","text = text.strip()\n","\n","# Remove extra spaces within the text\n","text = ' '.join(text.split())\n","\n","print(text)\n","\n"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text\n","import nltk\n","nltk.download('punkt')\n"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"I found a great website at https://www.example.com. Check it out!\"\n","no_url_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n","print(no_url_text)\n"],"metadata":{"id":"HSAK8GuPqIuC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove URLs from the text\n","import re\n","\n","text = \"I discovered an excellent website at https://www.example.com. Take a look!\"\n","removed_urls = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n","print(removed_urls)\n"],"metadata":{"id":"bZQvmyDyqZOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Tokenization\n","import nltk\n","nltk.download('punkt')\n","\n","text = \"Tokenization is an important step in natural language processing.\"\n","tokens = nltk.word_tokenize(text)\n","\n","print(tokens)\n"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n","import nltk\n","from nltk.stem import PorterStemmer\n","\n","stemmer = PorterStemmer()\n","\n","word = \"running\"\n","stemmed_word = stemmer.stem(word)\n","\n","print(stemmed_word)\n"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","word = \"running\"\n","lemmatized_word = lemmatizer.lemmatize(word)\n","\n","print(lemmatized_word)\n"],"metadata":{"id":"sRcnKvvdq22m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["The choice of text normalization techniques depends on the specific requirements of the NLP task and the characteristics of the dataset being processed. Different techniques may be more suitable for different scenarios. "],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","\n","text = \"I love reading books.\"\n","\n","# Tokenize the text into words\n","tokens = nltk.word_tokenize(text)\n","\n","# Perform POS tagging\n","pos_tags = nltk.pos_tag(tokens)\n","\n","print(pos_tags)\n"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"-qdCQy06sveP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vectorizing Text"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["Text vectorization is the process of converting textual data into numerical representations, or vectors, that can be processed by machine learning algorithms. It is a crucial step in natural language processing (NLP) tasks as most machine learning models require numerical inputs.\n","\n","There are several techniques for text vectorization. Here are three commonly used approaches:\n","\n","1.Bag-of-Words (BoW):\n","\n","\n","2.TF-IDF (Term Frequency-Inverse Document Frequency):\n","\n","\n","3.Word Embeddings:\n"],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","\n","df['Driving_License_Yes'].value_counts()"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting\n","# Dropping the 'Driving_License_Yes' column\n","df.drop(columns=['Driving_License_Yes'],axis=1,inplace=True)"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Since practically every customer has a driver's license, it is useless to insure anyone without one because it would be detrimental to the business. Hence, we wouldn't provide vehicle insurance to someone who didn't have a license to drive. As we can drop driving license column as they are not providing any valuable information."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["considered unimportant as practically every customer has a driver's license. Therefore, it doesn't provide valuable information for making predictions or analysis related to vehicle insurance."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["# Transform Your data"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data\n","# Normalizing the Dataset using Standard Scaling Technique.\n","\n","scaler=StandardScaler()\n","X_train=scaler.fit_transform(X_train)\n","X_test=scaler.transform(X_test)"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","# Train test split our data\n","\n","X_train,X_test,y_train,y_test = train_test_split(x_new,y_new, test_size=0.2,random_state=2)\n"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why? "],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["I have used 80-20 split ratio. This optimal ratio provides enough data for the model to train upon & also to upon.\n","This is a common splitting ratio used in machine learning, where a larger proportion of the data is used for training to ensure the model has enough data to learn from. The smaller proportion of data allocated for testing is used to evaluate the model's performance on unseen data, which helps to assess how well the model is generalizing to new data."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)\n","# Target variable countplot\n","sns.countplot(x=df['Response'], data=df, palette='mako')"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Defining X and y variable\n","\n","X=df.drop(['Response'],axis=1)\n","y=df['Response']"],"metadata":{"id":"emtqas0MuOqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Handling imbalanced dataset using SMOTE method\n","\n","sm=SMOTE()\n","x_new, y_new = sm.fit_resample(X, y.ravel())\n","\n","print(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1))) \n","print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y == 0))) \n","print(\"After OverSampling, counts of label '1': {}\".format(sum(y_new == 1))) \n","print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_new == 0))) \n","print('\\n')\n"],"metadata":{"id":"rhAa1hvAuO5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the balanced dataset\n","\n","plt.figure(figsize = (14,6))\n","plt.subplot(1,2,1)\n","sns.countplot(x=df['Response'], data=df, palette='husl')\n","plt.title('Before sampling',fontsize=20)\n","plt.subplot(1,2,2)\n","sns.countplot(x=y_new,palette='husl')\n","plt.title('After sampling',fontsize=20)\n","plt.show()"],"metadata":{"id":"7JgrWU69uPEP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["I have used SOMTE method to handle the class imbalance because it simply duplicates examples from the minority class in the training dataset prior to fitting a model. Although this balances the class distribution, it does not provide any additional information to the model."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","lr = LogisticRegression()\n","\n","# Fit the Algorithm\n","lr.fit(X_train,y_train)\n","\n","# Predict on the model\n","y_pred_lr = lr.predict(X_test)\n","y_pred_proba_lr = lr.predict_proba(X_test)[:,1]"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Evaluation\n","RS_lr= recall_score(y_test, y_pred_lr)\n","print(\"Recall_Score : \", RS_lr)\n","\n","PS_lr= precision_score(y_test, y_pred_lr)\n","print(\"Precision_Score :\",PS_lr)\n","\n","f1S_lr= f1_score(y_test, y_pred_lr)\n","print(\"f1_Score :\", f1S_lr)\n","\n","AS_lr= accuracy_score(y_test , y_pred_lr)\n","print(\"Accuracy_Score :\",AS_lr)\n","\n","acu_lr = roc_auc_score(y_test , y_pred_lr)\n","print(\"ROC_AUC Score:\",acu_lr)"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Visualizing evaluation Metric Score chart\n","# Confusion matrix\n","\n","cm_logistic = metrics.confusion_matrix(y_test, y_pred_lr)\n","print(cm_logistic)\n","print('\\n')\n","fig, ax = plot_confusion_matrix(conf_mat=cm_logistic, figsize=(6, 6), cmap=plt.cm.Blues)\n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ROC Curve\n","fpr, tpr, _ = roc_curve(y_test, y_pred_proba_lr)\n","plt.title('Logistic Regression ROC curve')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.plot(fpr,tpr)\n","plt.plot((0,1), linestyle=\"--\",color='black')\n","plt.show()"],"metadata":{"id":"M97a8XmTvc1R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The ML model used is a Logistic Regression model. The classification report shows the precision, recall, and F1-score for each class, as well as the support (number of instances) for each class in the training set.\n","\n","The precision is the ratio of true positive predictions to the total number of positive predictions. The recall is the ratio of true positive predictions to the total number of actual positive instances in the dataset. The F1-score is the harmonic mean of precision and recall.\n","\n","Looking at the evaluation metric scores, we can see that the model has an overall accuracy of 81%, meaning that it correctly classified 81% of the instances in the training set. The precision for class 0 is 76%, meaning that when the model predicted a class 0 instance, it was correct 76% of the time. The recall for class 0 is 89%, meaning that the model correctly identified 89% of the actual class 0 instances in the dataset. The F1-score for class 0 is 82%.\n","\n","Overall, the model seems to be performing reasonably well, with an accuracy of 81% on test set."],"metadata":{"id":"gUW8yPh6virh"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n","grid_lr = GridSearchCV(lr, param_grid, cv=5)\n","\n","# Fit the Algorithm\n","grid_lr.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_gcv = grid_lr.predict(X_test)"],"metadata":{"id":"tE5lQz3KvXtK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["**GridSearchCV** is a commonly used technique for hyperparameter tuning that involves searching over a predefined grid of hyperparameters and selecting the combination that gives the best performance on a validation set.\n","\n","In this case, the grid of hyperparameters included different values of C, which controls the regularization strength of the logistic regression model. The reason for using GridSearchCV is that it exhaustively searches over the entire grid of hyperparameters, which helps to find the optimal combination of hyperparameters that gives the best performance on the validation set."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"code","source":["# Evaluation\n","RS_lr_gcv= recall_score(y_test, y_pred_gcv)\n","print(\"Recall_Score : \", RS_lr_gcv)\n","\n","PS_lr_gcv= precision_score(y_test, y_pred_gcv)\n","print(\"Precision_Score :\",PS_lr_gcv)\n","\n","f1S_lr_gcv = f1_score(y_test, y_pred_gcv)\n","print(\"f1_Score :\", f1S_lr_gcv)\n","\n","AS_lr_gcv = accuracy_score(y_test , y_pred_gcv)\n","print(\"Accuracy_Score :\",AS_lr_gcv)\n","\n","acu_lr_gcv = roc_auc_score(y_test , y_pred_gcv)\n","print(\"ROC_AUC Score:\",acu_lr_gcv)"],"metadata":{"id":"9oeYI-jZv2Qw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Best cross-validation score:\", grid_lr.best_score_)\n","print(\"Best parameters:\", grid_lr.best_params_)"],"metadata":{"id":"cyLMej95v4hh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The best cross-validation score achieved was 0.81, and the best hyperparameter value for C was found to be 10.\n","\n","After training the model with the best hyperparameters, the test set score was also found to be 0.81. This suggests that the model is performing consistently well on both the training and test sets, and that it is unlikely to be overfitting.\n","\n","Overall, it appears that the logistic regression model with the selected hyperparameters is a good fit for the dataset, achieving an f1 score of 0.82."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# ML Model - 2 Implementation\n","rfc = RandomForestClassifier()\n","\n","# Fit the Algorithm\n","rfc.fit(X_train,y_train)\n","\n","# Predict on the model\n","y_pred_rfc = rfc.predict(X_test)\n","y_pred_proba_rfc = rfc.predict_proba(X_test)[:,1]"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Confusion Matrix\n","\n","cm_rf= metrics.confusion_matrix(y_test, y_pred_rfc)\n","print(cm_rf)\n","print('\\n')\n","fig, ax = plot_confusion_matrix(conf_mat=cm_rf, figsize=(6, 6), cmap=plt.cm.Blues)\n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"],"metadata":{"id":"4-O1V1nzx1cj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ROC Curve\n","\n","fpr, tpr, _ = roc_curve(y_test, y_pred_proba_rfc)\n","plt.title('Random Forest Classifier ROC curve')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.plot(fpr,tpr)\n","plt.plot((0,1), linestyle=\"--\",color='black')\n","plt.show()"],"metadata":{"id":"gSlMSwrHx1kF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","\n","clsr = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=30, max_features='log2',\n","                       max_leaf_nodes=40, max_samples=None,\n","                       min_impurity_decrease=0.0,\n","                       min_samples_leaf=1, min_samples_split=4,\n","                       min_weight_fraction_leaf=0.0, n_estimators=200,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)\n","\n","\n","# Fit the Algorithm\n","clsr.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_rfc_gcv = clsr.predict(X_test)"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["GridSearchCV is a commonly used technique for hyperparameter tuning that involves searching over a predefined grid of hyperparameters and selecting the combination that gives the best performance on a validation set.\n","\n","In this case, the grid of hyperparameters included different values of C, which controls the regularization strength of the logistic regression model. The reason for using GridSearchCV is that it exhaustively searches over the entire grid of hyperparameters, which helps to find the optimal combination of hyperparameters that gives the best performance on the validation set."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["The ML model used is Random Forest for classifier. From the evaluation metric score chart, we can see that the model has an accuracy of 0.86, which means that 86% of the predictions made by the model are correct. The precision for class 0 is 0.83, which means that out of all the positive predictions made for class 0, 83% of them are actually correct. The recall for class 1 is 0.91, which means that out of all the actual positive instances of class 1, the model correctly identified 91% of them. The F1-score for class 2 is 0.87, which is the harmonic mean of precision and recall, and provides an overall measure of the model's accuracy for that class.\n","\n","In summary, the Random Forest model has better performance on this classification task compared to logistic regression classifier."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"code","source":["# Evaluation\n","RS_rfc_gcv= recall_score(y_test, y_pred_rfc_gcv)\n","print(\"Recall_Score : \", RS_rfc_gcv)\n","\n","PS_rfc_gcv = precision_score(y_test, y_pred_rfc_gcv)\n","print(\"Precision_Score :\",PS_rfc_gcv)\n","\n","f1S_rfc_gcv = f1_score(y_test, y_pred_rfc_gcv)\n","print(\"f1_Score :\", f1S_rfc_gcv)\n","\n","AS_rfc_gcv = accuracy_score(y_test , y_pred_rfc_gcv)\n","print(\"Accuracy_Score :\",AS_rfc_gcv)\n","\n","acu_rfc_gcv = roc_auc_score(y_test , y_pred_rfc_gcv)\n","print(\"ROC_AUC Score:\",acu_rfc_gcv)"],"metadata":{"id":"KmZitIxMy4ym"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recall_Score :  0.9113799869195552\n","Precision_Score : 0.778600363179215\n","f1_Score : 0.839774011299435\n","Accuracy_Score : 0.8250822368421052\n","ROC_AUC Score: 0.8245682197999498"],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementation\n","\n","xgb = XGBClassifier()\n","\n","# Fit the Algorithm\n","xgb.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_xgb = xgb.predict(X_test)\n","y_pred_proba_xgb = xgb.predict_proba(X_test)[:,1]"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Evaluation\n","RS_xgb= recall_score(y_test, y_pred_xgb)\n","print(\"Recall_Score : \", RS_xgb)\n","\n","PS_xgb= precision_score(y_test, y_pred_xgb)\n","print(\"Precision_Score :\",PS_xgb)\n","\n","f1S_xgb = f1_score(y_test, y_pred_xgb)\n","print(\"f1_Score :\", f1S_xgb)\n","\n","AS_xgb = accuracy_score(y_test , y_pred_xgb)\n","print(\"Accuracy_Score :\",AS_xgb)\n","\n","acu_xgb = roc_auc_score(y_test , y_pred_xgb)\n","print(\"ROC_AUC Score:\",acu_xgb)"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Confusion Matrix\n","\n","cm_rf= metrics.confusion_matrix(y_test, y_pred_xgb)\n","print(cm_rf)\n","print('\\n')\n","fig, ax = plot_confusion_matrix(conf_mat=cm_rf, figsize=(6, 6), cmap=plt.cm.Blues)\n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"],"metadata":{"id":"TVub3IhIzeCM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ROC Curve\n","fpr, tpr, _ = roc_curve(y_test, y_pred_proba_xgb)\n","plt.title('Random Forest Classifier ROC curve')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.plot(fpr,tpr)\n","plt.plot((0,1), linestyle=\"--\",color='black')\n","plt.show()"],"metadata":{"id":"eRCHB-ibzg5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"code","source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","# Define the hyperparameter search space\n","\n","params = {\n","    'max_depth': [3, 5],\n","    'learning_rate': [0.1],\n","    'n_estimators': [10, 20],\n","}\n","\n","# Perform cross-validation and hyperparameter tuning\n","grid_search = GridSearchCV(xgb, params, cv=5, scoring='accuracy')\n","\n","# Fit the Algorithm\n","grid_search.fit(X_train, y_train)\n","\n","# Predict on the model\n","y_pred_xgb_gcv = grid_search.predict(X_test)"],"metadata":{"id":"eSVXuaSKpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["GridSearchCV is a commonly used technique for hyperparameter tuning that involves searching over a predefined grid of hyperparameters and selecting the combination that gives the best performance on a validation set.\n","\n","In this case, the grid of hyperparameters included different values of C, which controls the regularization strength of the logistic regression model. The reason for using GridSearchCV is that it exhaustively searches over the entire grid of hyperparameters, which helps to find the optimal combination of hyperparameters that gives the best performance on the validation set."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"code","source":["# Evaluation\n","RS_xgb_gcv= recall_score(y_test, y_pred_xgb_gcv)\n","print(\"Recall_Score : \", RS_xgb_gcv)\n","\n","PS_xgb_gcv = precision_score(y_test, y_pred_xgb_gcv)\n","print(\"Precision_Score :\",PS_xgb_gcv)\n","\n","f1S_xgb_gcv = f1_score(y_test, y_pred_xgb_gcv)\n","print(\"f1_Score :\", f1S_xgb_gcv)\n","\n","AS_xgb_gcv = accuracy_score(y_test , y_pred_xgb_gcv)\n","print(\"Accuracy_Score :\",AS_xgb_gcv)\n","\n","acu_xgb_gcv = roc_auc_score(y_test , y_pred_xgb_gcv)\n","print(\"ROC_AUC Score:\",acu_xgb_gcv)"],"metadata":{"id":"nrMAPGt80L1m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["I have chosen f1-score score as it is best able to explain the fit of the data by taking the harmonic mean of prescision score & recall score. F1-score is also the unweighted mean of these scores across all classes. In the best case, the macro average for precision, recall, and F1-score is 87%."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["I have chosen Random Forest Regressor as my final classifier model. With a f1-score score of 87%, we can consider random forest classifier as our best model."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"code","source":["features = x_new.columns\n","importances = rfc.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"FFMLoUSC0Vza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plotting figure\n","plt.figure(figsize=(6,6))\n","plt.style.use('dark_background')\n","plt.title('Feature Importance')\n","plt.barh(range(len(indices)), importances[indices], color='aqua', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","\n","plt.show()"],"metadata":{"id":"6Lz4hIuw0Xma"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see from the feature importance graph, the feature 'Previously_Insured_yes' can be considered as most important with relative importance of 0.2. The next 4 features are vintage, annual_premium, age and vehicle_damage-yes can be considered with relative importance ranging from 0.125-0.175. As these 5 main features play a role in decreasing the value of entropy, the machine learning model, random forest classifier considers them closer to the root node."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In our analysis, we initially performed EDA on all the features of our datset. We first analysed our dependent variable i.e, 'Response' and also transformed it. Next we analysed categorical variable and dropped the variable who had majority of one class. we also analysed numerical variable, check out the correlation, distribution and their relationship with the dependent variable. We then later hot encoded the categorical variables.\n","\n","Next we implemented 3 machine learning algorithms Logistic Regression, Random Forest Classifier, XG-Boost. We did some hyperparameter tuning to improve our model performance.\n","\n","Out of all above models Random forest Classifier gives the highest F1-score of 87% for test Set. No overfitting is seen.\n","\n","**So finally, the insurance company can deploy a machine learning model that uses Random Forest Classifier to predict the wheather the already existing health insurance customer would be interested in a vehicle insurance product. The company can improve the conversion rate by taking steps to encourage people to buy vehicle insurance by offering some incentives/ease of application & claim settlement process. Cross selling might be an effective way to increase the profits since the customer acquisition cost still remains 0.**"],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}